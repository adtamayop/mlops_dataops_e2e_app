{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validador de datasets\n",
    "\n",
    "With the statistics and schema in place, we can now validate our new dataset. The ExampleValidator pipeline component identifies anomalies in training and serving data. It can detect different classes of anomalies in the data.\n",
    "\n",
    "if  the  `ExampleValidator`  component  detects  a  misalignment  in  the  dataset  statistics or schema between the new and the previous dataset, it will set the status to failed inthe  metadata  store,  and  the  pipeline  ultimately  stops.  Otherwise,  the  pipeline  moves on to the next step, the data preprocessing\n",
    "\n",
    "TensorFlow Data Validation (TFDV) can be used to investigate and visualize your dataset. That includes looking at descriptive statistics, inferring a schema, checking for and fixing anomalies, and checking for drift and skew in our dataset. It's important to understand your dataset's characteristics, including how it might change over time in your production pipeline. It's also important to look for anomalies in your data, and to compare your training, evaluation, and serving datasets to make sure that they're consistent.\n",
    "\n",
    "TensorFlow Data Validation identifies anomalies in training and serving data, and can automatically create a schema by examining the data. The component can be configured to detect different classes of anomalies in the data. It can\n",
    "\n",
    "Perform validity checks by comparing data statistics against a schema that codifies expectations of the user.\n",
    "Detect training-serving skew by comparing examples in training and serving data.\n",
    "Detect data drift by looking at a series of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estadísitica descriptiva básica\n",
    "\n",
    "TFDV can compute descriptive statistics that provide a quick overview of the data in terms of the features that are present and the shapes of their value distributions.\n",
    "\n",
    "Internally, TFDV uses Apache Beam's data-parallel processing framework to scale the computation of statistics over large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generates from the first notebook \n",
    "train_path = \"./output_notebook_pipeline/CsvExampleGen/examples/1/Split-test/\"\n",
    "test_path =  \"./output_notebook_pipeline/CsvExampleGen/examples/1/Split-train/\"\n",
    "val_path =  \"./output_notebook_pipeline/CsvExampleGen/examples/1/Split-validation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = tfdv.generate_statistics_from_tfrecord(data_location=train_path)\n",
    "val_stats = tfdv.generate_statistics_from_tfrecord(data_location=val_path)\n",
    "tfdv.visualize_statistics(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de esquema de datos\n",
    "\n",
    "A schema defines constraints for the data that are relevant for ML. Example constraints include the data type of each feature, whether it's numerical or categorical, or the frequency of its presence in the data. For categorical features the schema also defines the domain - the list of acceptable values. Since writing a schema can be a tedious task, especially for datasets with lots of features, TFDV provides a method to generate an initial version of the schema based on the descriptive statistics.\n",
    "\n",
    "Getting the schema right is important because the rest of our production pipeline will be relying on the schema that TFDV generates to be correct\n",
    "\n",
    "The schema codifies properties which the input data is expected to satisfy, such as data types or categorical values, and can be modified or replaced by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(statistics=train_stats)\n",
    "tfdv.display_schema(schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check evaluation data for errors\n",
    "\n",
    "- It's important that our evaluation data is consistent with our training data, including that it uses the same schema. \n",
    "- It's also important that the evaluation data includes examples of roughly the same ranges of values for our numerical features as our training data, so that our coverage of the loss surface during evaluation is roughly the same as during training. \n",
    "- The same is true for categorical features. Otherwise, we may have training issues that are not identified during evaluation, because we didn't evaluate part of our loss surface.\n",
    "\n",
    "To detect unbalanced features in a Facets Overview, choose \"Non-uniformity\" from the \"Sort by\" dropdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_stats = tfdv.generate_statistics_from_tfrecord(data_location=test_path)\n",
    "\n",
    "# Compare evaluation data with training data\n",
    "tfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats,\n",
    "                          lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for evaluation anomalies\n",
    "\n",
    "Does our evaluation dataset match the schema from our training dataset? This is especially important for categorical features, where we want to identify the range of acceptable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check eval data for errors by validating the eval data stats using the previously inferred schema.\n",
    "anomalies = tfdv.validate_statistics(statistics=eval_stats, schema=schema)\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for drift and skew\n",
    "\n",
    "In addition to checking whether a dataset conforms to the expectations set in the schema, TFDV also provides functionalities to detect drift and skew. TFDV performs this check by comparing the statistics of the different datasets based on the drift/skew comparators specified in the schema\n",
    "\n",
    "* Drift\n",
    "\n",
    "Drift detection is supported for categorical features and between consecutive spans of data (i.e., between span N and span N+1), such as between different days of training data. We express drift in terms of L-infinity distance, and you can set the threshold distance so that you receive warnings when the drift is higher than is acceptable.\n",
    "\n",
    "Drift detection is supported between consecutive spans of data (i.e., between span N and span N+1), such as between different days of training data. We express drift in terms of L-infinity distance for categorical features and approximate Jensen-Shannon divergence for numeric features. You can set the threshold distance so that you receive warnings when the drift is higher than is acceptable. Setting the correct distance is typically an iterative process requiring domain knowledge and experimentation.\n",
    "\n",
    "\n",
    "* Skew\n",
    "\n",
    "TFDV can detect three different kinds of skew in your data - schema skew, feature skew, and distribution skew\n",
    "\n",
    "- Schema Skew\n",
    "\n",
    "Schema skew occurs when the training and serving data do not conform to the same schema. Both training and serving data are expected to adhere to the same schema. Any expected deviations between the two (such as the label feature being only present in the training data but not in serving) should be specified through environments field in the schema.\n",
    "\n",
    "- Feature Skew\n",
    "Feature skew occurs when the feature values that a model trains on are different from the feature values that it sees at serving time. For example, this can happen when:\n",
    "\n",
    "    A data source that provides some feature values is modified between training and serving time\n",
    "    \n",
    "    There is different logic for generating features between training and serving. For example, if you apply some transformation only in one of the two code    paths.\n",
    "\n",
    "* Distribution Skew\n",
    "\n",
    "Distribution skew occurs when the distribution of the training dataset is significantly different from the distribution of the serving dataset. One of the key causes for distribution skew is using different code or different data sources to generate the training dataset. Another reason is a faulty sampling mechanism that chooses a non-representative subsample of the serving data to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add skew comparator for 'close' feature.\n",
    "close = tfdv.get_feature(schema, 'label')\n",
    "close.skew_comparator.infinity_norm.threshold = 0.001\n",
    "\n",
    "# Add drift comparator for 'volume' feature\n",
    "volume=tfdv.get_feature(schema, 'col_t_1')\n",
    "volume.drift_comparator.infinity_norm.threshold = 0.001\n",
    "\n",
    "skew_anomalies = tfdv.validate_statistics(\n",
    "    train_stats, schema, \n",
    "    previous_statistics=val_stats,\n",
    "    serving_statistics=eval_stats)\n",
    "\n",
    "tfdv.display_anomalies(skew_anomalies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuando usar tfdv?\n",
    "\n",
    "It's easy to think of TFDV as only applying to the start of your training pipeline, as we did here, but in fact it has many uses. Here's a few more:\n",
    "\n",
    "- Validating new data for inference to make sure that we haven't suddenly started receiving bad features\n",
    "- Validating new data for inference to make sure that our model has trained on that part of the decision surface\n",
    "- Validating our data after we've transformed it and done feature engineering (probably using TensorFlow Transform) to make sure we haven't done something wrong\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
